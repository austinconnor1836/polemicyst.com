version: '3.9'
services:
  # Redis for job queuing
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    restart: unless-stopped

  # Clip Worker - Main job processing (Node.js)
  clip-worker:
    build:
      context: ./clip-worker
      dockerfile: ./Dockerfile
    environment:
      - REDIS_HOST=redis
      - NODE_ENV=production
    depends_on:
      - redis
      - ai-worker
    command: ["node", "dist/index.js"]
    restart: unless-stopped

  # AI Worker - Emotion and Sentiment Analysis (Python)
  ai-worker:
    build:
      context: ./ai-worker
      dockerfile: ./Dockerfile
    environment:
      - REDIS_HOST=redis
      - FLASK_APP=aiService.py
      - FLASK_RUN_HOST=0.0.0.0
      - FLASK_RUN_PORT=5000
    ports:
      - "5000:5000"
    depends_on:
      - redis
    restart: unless-stopped
  
  # Faster Whisper for transcription
  faster-whisper:
    image: lscr.io/linuxserver/faster-whisper:latest
    container_name: faster-whisper
    environment:
      - TZ=Etc/UTC
      - WHISPER_MODEL=tiny-int8
      - WHISPER_LANG=en
    volumes:
      - ./faster-whisper-data:/config
    ports:
      - '10300:10300'
    restart: unless-stopped

  # Ollama for language model processing
  ollama:
    build:
      context: ./backend
      dockerfile: ollama.Dockerfile
    container_name: ollama
    volumes:
      - ./backend/scripts/start-ollama.sh:/start.sh
      - ollama-data:/root/.ollama
    entrypoint: ["/bin/bash", "/start.sh"]
    ports:
      - "11434:11434"
    restart: unless-stopped

volumes:
  ollama-data:
  faster-whisper-data:
