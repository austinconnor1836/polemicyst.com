version: '3.9'
services:
  db:
    image: postgres:15
    container_name: clips-genie-db
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: clips-genie
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    volumes:
      - pg-data:/var/lib/postgresql/data
    networks:
      - app-network
  
  # Redis for job queuing
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    restart: unless-stopped
    networks:
      - app-network
  
  api:
    build:
      context: .
      dockerfile: ./backend/api/Dockerfile
    command: ["node", "dist/index.js"]
    ports:
      - "3001:3001"
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/clips-genie
      - REDIS_HOST=redis
    depends_on:
      - db
      - redis
    networks:
      - app-network

  # Clip Worker - Main job processing (Node.js)
  clip-worker:
    build:
      context: .
      dockerfile: ./backend/workers/clip-worker/Dockerfile
    environment:
      - REDIS_HOST=redis
      - NODE_ENV=production
    depends_on:
      - redis
    command: ["node", "dist/index.js"]
    restart: unless-stopped
    networks:
      - app-network
  
  clip-metadata-worker:
    build:
      context: .
      dockerfile: ./backend/workers/clip-worker/clip-metadata-worker.Dockerfile
    # command: ["node", "dist/workers/clip-worker/generate/generateMetadataWorker.js"]
    command: ["node", "dist/index.js"]
    environment:
      - REDIS_HOST=redis
      - NODE_ENV=production
    depends_on:
      - redis
      - db
    restart: unless-stopped
    networks:
      - app-network


  # Faster Whisper for transcription
  faster-whisper:
    image: lscr.io/linuxserver/faster-whisper:latest
    container_name: faster-whisper
    environment:
      - TZ=Etc/UTC
      - WHISPER_MODEL=tiny-int8
      - WHISPER_LANG=en
    volumes:
      - ./faster-whisper-data:/config
    ports:
      - '10300:10300'
    restart: unless-stopped
    networks:
      - app-network

  # Ollama for language model processing
  ollama:
    build:
      context: .
      dockerfile: ./backend/workers/ollama-worker/Dockerfile
    container_name: ollama
    volumes:
      - ./scripts/start-ollama.sh:/start.sh
      - ollama-data:/root/.ollama
    # entrypoint: ["/bin/bash", "/start.sh"]
    command: ["./scripts/start-ollama.sh"]
    ports:
      - "11434:11434"
    restart: unless-stopped
    networks:
      - app-network
  
  poller:
    build:
      context: .
      # dockerfile: ./workers/workers.Dockerfile
      dockerfile: ./backend/workers/poller-worker/Dockerfile
    # entrypoint: ["/bin/bash", "/start.sh"]
    command: ["./start.sh"]
    volumes:
      - ./shared:/app/shared  # ⬅️ needed for prisma and shared code
      - ./prisma:/app/prisma
    depends_on:
      - backend
      - redis
    environment:
      - NODE_ENV=production
    env_file:
      - .env
    restart: unless-stopped
    networks:
      - app-network

volumes:
  ollama-data:
  faster-whisper-data:
  pg-data:

networks:
  app-network: